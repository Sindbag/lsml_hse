{"nbformat_minor": 2, "cells": [{"source": "# Fix all jupyter notebook problems\n\n<sub><sub>0. Pray </sub></sub>\n1. Connect to head machine via SSH\n2. Open `/usr/bin/anaconda/lib/python2.7/site-packages/nbformat/_version.py` and change 5 to 4.\n3. Fix anaconda installation via official fix script. \n```\ncurl https://gregorysfixes.blob.core.windows.net/public/fix-conda.sh | sudo sh\n```\n4. Install all necessary python packages. At least hdfs and kaggle - \n```\nsudo /usr/bin/anaconda/bin/conda install -c conda-forge python-hdfs kaggle --yes\n```\n5. Open Ambari and restart jupyter service.\n6. Open azure jupyter notebook and upload this notebook\n7. Check, that cells below can be executed correctly", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "%%local\nimport hdfs", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 10.109130859375, "end_time": 1582221151133.349}}, "collapsed": false}}, {"source": "# Create Spark Context", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "sc = spark.sparkContext", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 49.291015625, "end_time": 1582221292457.491}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "import pandas as pd\nfrom pyspark.sql import SparkSession\n\nss = SparkSession(sc)", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3278.9638671875, "end_time": 1582221295838.154}}, "editable": true, "collapsed": true, "deletable": true}}, {"execution_count": 16, "cell_type": "code", "source": "hadoop = sc._jvm.org.apache.hadoop\nfs = hadoop.fs.FileSystem\nconf = hadoop.conf.Configuration() \npath = hadoop.fs.Path('/')\n    \ndef hdfs_ls(path):\n    result = []\n    for f in fs.get(conf).listStatus(hadoop.fs.Path(path)):\n        result.append(str(f.getPath()))\n    return result", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 247.760009765625, "end_time": 1582221605573.014}}, "collapsed": false}}, {"execution_count": 12, "cell_type": "code", "source": "%%local\nfrom hdfs import InsecureClient\nhdfs_client = InsecureClient(\"http://cluster1:50070\", user='hdfs')", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 3.193115234375, "end_time": 1582221574672.53}}, "editable": true, "collapsed": false, "deletable": true}}, {"execution_count": 17, "cell_type": "code", "source": "hdfs_ls('/')", "outputs": [{"output_type": "stream", "name": "stdout", "text": "['wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/custom-scriptaction-logs', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/app-logs', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/tmp', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/warehouse', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/mapred', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HDInsight_TestAccessiblityBlobName', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hive', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/example', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/apps', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HdiSamples', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/yarn', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/HdiNotebooks', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/amshbase', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/ams', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/atshistory', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hbase', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/hdp', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/user', 'wasb://spark2-fix-hw@hadoop2hdistorage2.blob.core.windows.net/mr-history']"}], "metadata": {"cell_status": {"execute_time": {"duration": 246.380859375, "end_time": 1582221606767.212}}, "collapsed": false}}, {"source": "# Download task data\n\nDownload data directly from kaggle. Read this to understand how: https://github.com/Kaggle/kaggle-api", "cell_type": "markdown", "metadata": {}}, {"execution_count": 49, "cell_type": "code", "source": "%%local\n! cat ~/.kaggle/kaggle.json", "outputs": [{"output_type": "stream", "name": "stdout", "text": "{\"username\": \"alexeykosmos\", \"key\": \"750a605c53f61211eeea14909ed129b6\"}"}], "metadata": {"cell_status": {"execute_time": {"duration": 210.26806640625, "end_time": 1582219662750.281}}, "collapsed": false}}, {"execution_count": 65, "cell_type": "code", "source": "%%local\n! kaggle competitions files outbrain-click-prediction", "outputs": [{"output_type": "stream", "name": "stdout", "text": "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/spark/.kaggle/kaggle.json'\nname                           size  creationDate         \n----------------------------  -----  -------------------  \nclicks_test.csv.zip           135MB  2018-06-22 05:33:10  \ndocuments_entities.csv.zip    126MB  2018-06-22 05:33:10  \ndocuments_topics.csv.zip      121MB  2018-06-22 05:33:10  \ndocuments_categories.csv.zip   32MB  2018-06-22 05:33:10  \npage_views_sample.csv.zip     149MB  2018-06-22 05:33:10  \nclicks_train.csv.zip          390MB  2018-06-22 05:33:10  \npage_views.csv.zip             35GB  2018-06-22 05:33:10  \nevents.csv.zip                478MB  2018-06-22 05:33:10  \nsample_submission.csv.zip     100MB  2018-06-22 05:33:10  \npromoted_content.csv.zip        3MB  2018-06-22 05:33:10  \ndocuments_meta.csv.zip         16MB  2018-06-22 05:33:10  \n"}], "metadata": {"cell_status": {"execute_time": {"duration": 1054.541015625, "end_time": 1582220036186.809}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! kaggle competitions download outbrain-click-prediction", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 943.577880859375, "end_time": 1582220497101.169}}, "collapsed": false}}, {"source": "# Load data to HDFS", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "https://www.kaggle.com/c/outbrain-click-prediction/data", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! hdfs dfs -rm -r /task1", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 2680.4541015625, "end_time": 1582220881691.037}}, "collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! for i in `ls *.zip`; do unzip -p $i | tqdm | hdfs dfs -put - /task1/$i; done", "outputs": [], "metadata": {"cell_status": {"execute_time": {"duration": 4578.701904296875, "end_time": 1582220873264.359}}, "collapsed": false}}, {"execution_count": 39, "cell_type": "code", "source": "%%local\n! hdfs dfs -du -s -h /task1/*.csv", "outputs": [{"output_type": "stream", "name": "stdout", "text": "483.5 M  /task1/clicks_test.csv\n1.4 G  /task1/clicks_train.csv\n112.5 M  /task1/documents_categories.csv\n309.1 M  /task1/documents_entities.csv\n85.2 M  /task1/documents_meta.csv\n323.7 M  /task1/documents_topics.csv\n1.1 G  /task1/events.csv\n88.4 G  /task1/page_views.csv\n433.3 M  /task1/page_views_sample.csv\n13.2 M  /task1/promoted_content.csv\n260.5 M  /task1/sample_submission.csv\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Read example", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "pvdf = ss.read.csv(\"/task1/page_views.csv\", header=True)", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "pvdf.dtypes", "outputs": [{"execution_count": 13, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "pvdf.show(5)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------+-----------+---------+--------+------------+--------------+\n|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n+--------------+-----------+---------+--------+------------+--------------+\n|1fd5f051fba643|        120| 31905835|       1|          RS|             2|\n|8557aa9004be3b|        120| 32053104|       1|       VN>44|             2|\n|c351b277a358f0|        120| 54013023|       1|       KR>12|             1|\n|8205775c5387f9|        120| 44196592|       1|       IN>16|             2|\n|9cb0ccd8458371|        120| 65817371|       1|   US>CA>807|             2|\n+--------------+-----------+---------+--------+------------+--------------+\nonly showing top 5 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%time\npvdf.count()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 76 ms, sys: 20 ms, total: 96 ms\nWall time: 9min 16s\n"}, {"execution_count": 15, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Parquet is faster than CSV", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "http://events.linuxfoundation.org/sites/events/files/slides/ApacheCon%20BigData%20Europe%202016%20-%20Parquet%20in%20Practice%20%26%20Detail_0.pdf", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%time\npvdf.write.parquet(\"/task1/page_views.parquet\")", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 31, "cell_type": "code", "source": "%%local\n! hdfs dfs -du -s -h /task1/page_views.parquet", "outputs": [{"output_type": "stream", "name": "stdout", "text": "47.3 G  /task1/page_views.parquet\r\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 33, "cell_type": "code", "source": "pvdf2 = ss.read.parquet(\"/task1/page_views.parquet\")", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 35, "cell_type": "code", "source": "%%time\nfrom IPython.display import display\nboo = pvdf2.groupBy(\"geo_location\").count().collect()\ndisplay(boo[:5])", "outputs": [{"output_type": "display_data", "data": {}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\nWall time: 22.2 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 36, "cell_type": "code", "source": "%%time\nboo = pvdf.groupBy(\"geo_location\").count().collect()\ndisplay(boo[:5])", "outputs": [{"output_type": "display_data", "data": {}, "metadata": {}}, {"output_type": "stream", "name": "stdout", "text": "CPU times: user 96 ms, sys: 20 ms, total: 116 ms\nWall time: 10min 34s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Convert all to Parquet", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 58, "cell_type": "code", "source": "%%time\ndef convert_all_to_parquet():\n    task_dir = \"/task1/\"\n    all_files = hdfs_ls(task_dir)\n    for fn in all_files:\n        if fn.endswith(\".csv\"):\n            fn_after = fn.replace(\".csv\", \".parquet\")\n            path_before = task_dir + fn\n            path_after = task_dir + fn_after\n            if fn_after not in all_files:\n                # generate parquet\n                df = ss.read.csv(path_before, header=True)\n                df.write.parquet(path_after)\n            # remove csv, we have parquet now\n            print(fn_after, \"done\")\n\nconvert_all_to_parquet()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "clicks_test.parquet done\nclicks_train.parquet done\ndocuments_categories.parquet done\ndocuments_entities.parquet done\ndocuments_meta.parquet done\ndocuments_topics.parquet done\nevents.parquet done\npage_views.parquet done\npage_views_sample.parquet done\npromoted_content.parquet done\nsample_submission.parquet done\nCPU times: user 96 ms, sys: 0 ns, total: 96 ms\nWall time: 4min 37s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%local\n! hdfs dfs -rm /task1/*.csv", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": 60, "cell_type": "code", "source": "%%local\n! hdfs dfs -du -s -h /task1/*", "outputs": [{"output_type": "stream", "name": "stdout", "text": "133.2 M  /task1/clicks_test.parquet\n367.5 M  /task1/clicks_train.parquet\n36.5 M  /task1/documents_categories.parquet\n184.0 M  /task1/documents_entities.parquet\n21.2 M  /task1/documents_meta.parquet\n183.3 M  /task1/documents_topics.parquet\n669.3 M  /task1/events.parquet\n47.3 G  /task1/page_views.parquet\n236.9 M  /task1/page_views_sample.parquet\n5.0 M  /task1/promoted_content.parquet\n184.2 M  /task1/sample_submission.parquet\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# Preview all files", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 65, "cell_type": "code", "source": "%%time\ndef preview_all_files():\n    task_dir = \"/task1/\"\n    all_files = hdfs_ls(task_dir)\n    for fn in all_files:\n        df = ss.read.parquet(task_dir + fn)\n        print()\"#\" * 15 + \" {0} \".format(task_dir + fn) + \"#\" * 15)\n        df.show(1)\n        \npreview_all_files()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "############### /task1/clicks_test.parquet ###############\n+----------+------+\n|display_id| ad_id|\n+----------+------+\n|  17805143|288388|\n+----------+------+\nonly showing top 1 row\n\n############### /task1/clicks_train.parquet ###############\n+----------+-----+-------+\n|display_id|ad_id|clicked|\n+----------+-----+-------+\n|         1|42337|      0|\n+----------+-----+-------+\nonly showing top 1 row\n\n############### /task1/documents_categories.parquet ###############\n+-----------+-----------+----------------+\n|document_id|category_id|confidence_level|\n+-----------+-----------+----------------+\n|    1544588|       1513|     0.263546236|\n+-----------+-----------+----------------+\nonly showing top 1 row\n\n############### /task1/documents_entities.parquet ###############\n+-----------+--------------------+-----------------+\n|document_id|           entity_id| confidence_level|\n+-----------+--------------------+-----------------+\n|    1539011|e01ed0c4a3e8f8f35...|0.327269624728567|\n+-----------+--------------------+-----------------+\nonly showing top 1 row\n\n############### /task1/documents_meta.parquet ###############\n+-----------+---------+------------+-------------------+\n|document_id|source_id|publisher_id|       publish_time|\n+-----------+---------+------------+-------------------+\n|     325048|      822|         253|2013-02-27 00:00:00|\n+-----------+---------+------------+-------------------+\nonly showing top 1 row\n\n############### /task1/documents_topics.parquet ###############\n+-----------+--------+------------------+\n|document_id|topic_id|  confidence_level|\n+-----------+--------+------------------+\n|     801028|     280|0.0148711250868194|\n+-----------+--------+------------------+\nonly showing top 1 row\n\n############### /task1/events.parquet ###############\n+----------+--------------+-----------+---------+--------+------------+\n|display_id|          uuid|document_id|timestamp|platform|geo_location|\n+----------+--------------+-----------+---------+--------+------------+\n|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n+----------+--------------+-----------+---------+--------+------------+\nonly showing top 1 row\n\n############### /task1/page_views.parquet ###############\n+--------------+-----------+---------+--------+------------+--------------+\n|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n+--------------+-----------+---------+--------+------------+--------------+\n|68fb8eb72c49c4|    1201414| 63621328|       3|       GB>F8|             2|\n+--------------+-----------+---------+--------+------------+--------------+\nonly showing top 1 row\n\n############### /task1/page_views_sample.parquet ###############\n+--------------+-----------+---------+--------+------------+--------------+\n|          uuid|document_id|timestamp|platform|geo_location|traffic_source|\n+--------------+-----------+---------+--------+------------+--------------+\n|7504d9623fdc7e|        234| 72194818|       1|   US>CA>825|             1|\n+--------------+-----------+---------+--------+------------+--------------+\nonly showing top 1 row\n\n############### /task1/promoted_content.parquet ###############\n+-----+-----------+-----------+-------------+\n|ad_id|document_id|campaign_id|advertiser_id|\n+-----+-----------+-----------+-------------+\n|    1|       6614|          1|            7|\n+-----+-----------+-----------+-------------+\nonly showing top 1 row\n\n############### /task1/sample_submission.parquet ###############\n+----------+--------------------+\n|display_id|               ad_id|\n+----------+--------------------+\n|  21960532|50582 190398 2293...|\n+----------+--------------------+\nonly showing top 1 row\n\nCPU times: user 28 ms, sys: 4 ms, total: 32 ms\nWall time: 2.47 s\n"}], "metadata": {"scrolled": false, "collapsed": false, "editable": true, "deletable": true}}, {"source": "# Register all tables to be usable in SQL queries", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "%%time\ndef register_all_tables():\n    task_dir = \"/task1/\"\n    all_files = hdfs_ls(task_dir)\n    for fn in all_files:\n        if fn.endswith(\".parquet\"):\n            table_name = fn.replace(\".parquet\", \"\")\n            df = ss.read.parquet(task_dir + fn)\n            df.registerTempTable(table_name)\n            print table_name, \"done\"\n        \nregister_all_tables()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "clicks_test done\nclicks_train done\ndocuments_categories done\ndocuments_categories2 done\ndocuments_entities done\ndocuments_meta done\ndocuments_topics done\nevents done\nevents_test done\nevents_train done\njoined_events done\npage_views done\npage_views_sample done\npromoted_content done\nsample_submission done\nCPU times: user 36 ms, sys: 8 ms, total: 44 ms\nWall time: 18.2 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# SQL query example", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 72, "cell_type": "code", "source": "%%time\nss.sql(\"\"\"\nselect count(distinct(uuid)) as users_count\nfrom events\n\"\"\").collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 16.1 s\n"}, {"execution_count": 72, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "# 1. Baseline", "cell_type": "markdown", "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"source": "Simple model using the following features:\n- **clicked**\n- geo_location features (country, state, dma)\n- day_of_week (from timestamp, use *date.isoweekday()*)\n- ad_id\n- ad_document_id\n- campaign_id\n- advertiser_id\n- display_document_id\n- platform", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## Calculate features for VW", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "- Use DataFrame API to join tables (functions in SQL queries: https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html)\n- Use Python API to calculate features and save them as text for VW (*saveAsTextFile()*)\n- Hash features in Spark (24 bits, use *sklearn.utils.murmurhash.murmurhash3_32*)\n- Split dataset in Spark into 90% train, 10% test **by display_id**, save the split for further use", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 5, "cell_type": "code", "source": "from sklearn.utils.murmurhash import murmurhash3_32\ndef hasher(x, bits):\n    return murmurhash3_32(x) % 2**bits", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"source": "## Copy data from HDFS to cluster1 machine", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "We will run vowpal wabbit **locally**, need to copy data from HDFS", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 2, "cell_type": "code", "source": "import os\nimport shutil\n\ndef copy_text_to_local(hdfs_path, local_path):\n    if os.path.exists(local_path):\n        shutil.rmtree(local_path)\n    os.mkdir(local_path)\n    os.system('hdfs dfs -getmerge \"{0}/*\" {1}'.format(hdfs_path, local_path))\n    os.system('cat {0}/part-* > {1}'.format(local_path, local_path + \"/merged.txt\"))\n    print \"done\"", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%time\ncopy_text_to_local(\"/task1/train.txt\", \"/data/train.txt\")", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": 3, "cell_type": "code", "source": "! ls -lh /data/train.txt/merged.txt", "outputs": [{"output_type": "stream", "name": "stdout", "text": "-rw-rw-r-- 1 ubuntu ubuntu 9.2G Apr 23 11:31 /data/train.txt/merged.txt\r\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "%%time\ncopy_text_to_local(\"/task1/test.txt\", \"/data/test.txt\")", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": 4, "cell_type": "code", "source": "! ls -lh /data/test.txt/merged.txt", "outputs": [{"output_type": "stream", "name": "stdout", "text": "-rw-rw-r-- 1 ubuntu ubuntu 1.1G Apr 23 11:32 /data/test.txt/merged.txt\r\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "## Install VW", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "! sudo apt-get install libboost-program-options-dev zlib1g-dev libboost-python-dev libtool m4 automake -y", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "! wget https://github.com/JohnLangford/vowpal_wabbit/archive/8.2.0.tar.gz", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "! tar -xzf 8.2.0.tar.gz", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"source": "cd vowpal_wabbit-8.2.0\n\n./autogen.sh\n\nmake -j4\n\nmake test -j4\n\nsudo make install", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "## Train VW", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "https://github.com/JohnLangford/vowpal_wabbit/wiki/Command-line-arguments", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 10, "cell_type": "code", "source": "! head -n2 /data/train.txt/merged.txt", "outputs": [{"output_type": "stream", "name": "stdout", "text": "-1 |f 5611969:1.0 11418299:1.0 11602085:1.0 15617356:1.0 15750989:1.0 786735:1.0 3083696:1.0 4204498:1.0 5728439:1.0 1376507:1.0\r\n1 |f 1748258:1.0 1376507:1.0 11602085:1.0 14125547:1.0 15617356:1.0 16621393:1.0 4204498:1.0 9609462:1.0 5728439:1.0 11418299:1.0\r\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 12, "cell_type": "code", "source": "%%time\n! LD_LIBRARY_PATH=/usr/local/lib vw -d /data/train.txt/merged.txt -b 24 -c -k --ftrl --passes 1 -f /data/model --holdout_off --loss_function logistic --random_seed 42 --progress 8000000 ", "outputs": [{"output_type": "stream", "name": "stdout", "text": "final_regressor = /data/model\nEnabling FTRL based optimization\nAlgorithm used: Proximal-FTRL\nftrl_alpha = 0.005\nftrl_beta = 0.1\nNum weight bits = 24\nlearning rate = 0.5\ninitial_t = 0\npower_t = 0.5\ncreating cache_file = /data/train.txt/merged.txt.cache\nReading datafile = /data/train.txt/merged.txt\nnum sources = 1\naverage  since         example        example  current  current  current\nloss     last          counter         weight    label  predict features\n0.459597 0.459597      8000000      8000000.0  -1.0000  -1.4509       11\n0.454955 0.450313     16000000     16000000.0  -1.0000   0.2849       11\n0.452520 0.447649     24000000     24000000.0  -1.0000  -1.0289       11\n0.450893 0.446015     32000000     32000000.0  -1.0000  -1.5837       11\n0.449772 0.445288     40000000     40000000.0  -1.0000  -1.2465       11\n0.448888 0.444469     48000000     48000000.0  -1.0000  -1.6476       11\n0.448170 0.443861     56000000     56000000.0  -1.0000  -1.0652        9\n0.447529 0.443043     64000000     64000000.0  -1.0000  -1.6281       11\n0.447042 0.443142     72000000     72000000.0  -1.0000  -0.9352       11\n\nfinished run\nnumber of examples = 78435295\nweighted example sum = 78435295.000000\nweighted label sum = -48058469.000000\naverage loss = 0.446692\nbest constant = -1.426513\nbest constant's loss = 0.491462\ntotal feature number = 839234743\nCPU times: user 2.56 s, sys: 984 ms, total: 3.54 s\nWall time: 2min 57s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "## Check VW test performance", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 13, "cell_type": "code", "source": "%%time\n! LD_LIBRARY_PATH=/usr/local/lib vw -d /data/test.txt/merged.txt -i /data/model -t -k -p /data/test_predictions.txt --progress 1000000 --link=logistic", "outputs": [{"output_type": "stream", "name": "stdout", "text": "only testing\npredictions = /data/test_predictions.txt\nNum weight bits = 24\nlearning rate = 0.5\ninitial_t = 0\npower_t = 0.5\nusing no cache\nReading datafile = /data/test.txt/merged.txt\nnum sources = 1\naverage  since         example        example  current  current  current\nloss     last          counter         weight    label  predict features\n1.833906 1.833906      1000000      1000000.0  -1.0000   0.1692       11\n1.834238 1.834571      2000000      2000000.0  -1.0000   0.1054       11\n1.834516 1.835071      3000000      3000000.0  -1.0000   0.1948       11\n1.834247 1.833440      4000000      4000000.0  -1.0000   0.0511       11\n1.833725 1.831640      5000000      5000000.0  -1.0000   0.4039       11\n1.834064 1.835758      6000000      6000000.0  -1.0000   0.3104       10\n1.833605 1.830850      7000000      7000000.0   1.0000   0.2100       11\n1.833540 1.833085      8000000      8000000.0   1.0000   0.2825       11\n\nfinished run\nnumber of examples per pass = 8706436\npasses used = 1\nweighted example sum = 8706436.000000\nweighted label sum = -5334076.000000\naverage loss = 1.833502\nbest constant = -0.612659\nbest constant's loss = 0.624649\ntotal feature number = 93153050\nCPU times: user 456 ms, sys: 148 ms, total: 604 ms\nWall time: 33.9 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 15, "cell_type": "code", "source": "import numpy as np\n\ndef read_vw_predictions(p):\n    y_pred = []\n    with open(p, \"r\") as f:\n        for line in f:\n            y_pred.append(float(line.split()[0]))\n    return np.array(y_pred)\n\ny_pred = read_vw_predictions(\"/data/test_predictions.txt\")", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 16, "cell_type": "code", "source": "def get_vw_y_true(p):\n    y_true = []\n    with open(p, \"r\") as f:\n        for line in f:\n            y_true.append(float(line.partition(\" \")[0]))\n    return np.array(y_true)\n\ny_true = get_vw_y_true(\"/data/test.txt/merged.txt\")", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": 17, "cell_type": "code", "source": "from sklearn.metrics import log_loss, roc_auc_score", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": 18, "cell_type": "code", "source": "log_loss(y_true, y_pred)", "outputs": [{"execution_count": 18, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 19, "cell_type": "code", "source": "roc_auc_score(y_true, y_pred)", "outputs": [{"execution_count": 19, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "## Make submission to Kaggle", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"source": "# 2. Better model", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "This time let's make a personalized recommender using:\n- page views information\n- document properties\n\nIdeas for features:\n- uuid topic, entity, publisher, ... preferences\n- document similarities\n- ...", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"source": "## More SQL examples", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 10, "cell_type": "code", "source": "# we start with a DataFrame\nevents_df = ss.sql(\"select * from events\")\nevents_df.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+--------------+-----------+---------+--------+------------+\n|display_id|          uuid|document_id|timestamp|platform|geo_location|\n+----------+--------------+-----------+---------+--------+------------+\n|         1|cb8c55702adb93|     379743|       61|       3|   US>SC>519|\n|         2|79a85fa78311b9|    1794259|       81|       2|   US>CA>807|\n|         3|822932ce3d8757|    1179111|      182|       2|   US>MI>505|\n+----------+--------------+-----------+---------+--------+------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 23, "cell_type": "code", "source": "# we can make RDD of Rows with *.rdd\nfrom pyspark.sql import Row\nevents_df.rdd.take(3)", "outputs": [{"execution_count": 23, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 30, "cell_type": "code", "source": "# When it's RDD, we can use Python to create new RDD of Rows\n(\n    events_df.rdd\n    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n).take(3)", "outputs": [{"execution_count": 30, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 31, "cell_type": "code", "source": "# we can convert it back to DataFrame if it's still a table that can be converted to Java types\nss.createDataFrame(\n    events_df.rdd\n    .map(lambda x: Row(foo=x.geo_location.split(\">\"), bar=x.uuid))\n).show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------+-------------+\n|           bar|          foo|\n+--------------+-------------+\n|cb8c55702adb93|[US, SC, 519]|\n|79a85fa78311b9|[US, CA, 807]|\n|822932ce3d8757|[US, MI, 505]|\n+--------------+-------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 36, "cell_type": "code", "source": "%%time\n# we can save it to HDFS as parquet (if it's a DataFrame)\nss.createDataFrame(\n    events_df.rdd\n    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n).write.mode(\"overwrite\").parquet(\"/task1/example1\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 36 ms, sys: 8 ms, total: 44 ms\nWall time: 3min 4s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 39, "cell_type": "code", "source": "ss.read.parquet(\"/task1/example1\").printSchema()\nss.read.parquet(\"/task1/example1\").show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "root\n |-- bar: string (nullable = true)\n |-- foo: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n+--------------+-------------+\n|           bar|          foo|\n+--------------+-------------+\n|cb8c55702adb93|[US, SC, 519]|\n|79a85fa78311b9|[US, CA, 807]|\n|822932ce3d8757|[US, MI, 505]|\n+--------------+-------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 41, "cell_type": "code", "source": "%%time\n# or we can skip DataFrame API if we use Python functions (there will be no speed increase)\n(\n    events_df.rdd\n    .map(lambda x: Row(foo=x.geo_location.split(\">\") if x.geo_location else [], bar=x.uuid))\n).saveAsPickleFile(\"/task1/example2\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 36 ms, sys: 0 ns, total: 36 ms\nWall time: 2min 41s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 43, "cell_type": "code", "source": "sc.pickleFile(\"/task1/example2\").take(3)", "outputs": [{"execution_count": 43, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 49, "cell_type": "code", "source": "# sometimes we cannot make a DataFrame\nimport numpy as np\nrdd = (\n    events_df.rdd\n    .map(lambda x: Row(x=np.array(x.geo_location.split(\">\") if x.geo_location else [])))\n)\nrdd.take(2)", "outputs": [{"execution_count": 49, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "# throws TypeError: not supported type: <type 'numpy.ndarray'>\nss.createDataFrame(rdd)", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 52, "cell_type": "code", "source": "%%time\n# but we can save as RDD in pickle file just fine\nrdd.saveAsPickleFile(\"/task1/example3\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 28 ms, sys: 8 ms, total: 36 ms\nWall time: 3min 31s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"source": "Takeaways:\n- use DataFrames when you can (simple join's, select's, groupby's), it will be faster\n- use RDD and Python when you can't use DataFrame API\n- convert it back to DataFrame if needed\n- or save to pickles (can save almost any Python object as pickle)", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "# Built-in SQL functions", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"source": "You can find more at https://spark.apache.org/docs/2.1.0/api/java/org/apache/spark/sql/functions.html", "cell_type": "markdown", "metadata": {"editable": true, "deletable": true}}, {"execution_count": 53, "cell_type": "code", "source": "# sql version\ndf = ss.sql(\"\"\"\nselect\n    document_id,\n    collect_list(struct(category_id, confidence_level)) as categories\nfrom\n    documents_categories\ngroup by document_id\n\"\"\")\ndf.show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+-----------+--------------------+\n|document_id|          categories|\n+-----------+--------------------+\n|     100010|[[1513,0.79842798...|\n|    1000240|[[1505,0.92], [15...|\n|    1000280|[[1909,0.92], [19...|\n+-----------+--------------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 55, "cell_type": "code", "source": "%%time\ndf.write.mode(\"overwrite\").parquet(\"/task1/example4\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 12.5 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 57, "cell_type": "code", "source": "# or we can use RDD and Python if we are not aware of those SQL functions\nrdd = (\n    ss.sql(\"select * from documents_categories\")\n    .rdd\n    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n    .groupByKey()\n    .map(lambda (k, vs): (k, list(vs)))\n)\nrdd.take(3)", "outputs": [{"execution_count": 57, "output_type": "execute_result", "data": {}, "metadata": {}}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 58, "cell_type": "code", "source": "%%time\n# it's much slower, but we can do almost everything in Python\nrdd.saveAsPickleFile(\"/task1/example5\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 28 ms, sys: 12 ms, total: 40 ms\nWall time: 3min 31s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 63, "cell_type": "code", "source": "# but sometimes with Python we can do more\nrdd = (\n    ss.sql(\"select * from documents_categories\")\n    .rdd\n    .map(lambda x: (x.document_id, (x.category_id, x.confidence_level)))\n    .groupByKey()\n    .map(lambda (k, vs): Row(document_id=k, categories={a: float(b) for a, b in vs}))\n)", "outputs": [], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 60, "cell_type": "code", "source": "%%time\n# much faster thanks to conversion back to DataFrame (works for simple python collections in columns)\nss.createDataFrame(rdd).write.parquet(\"/task1/example6\")", "outputs": [{"output_type": "stream", "name": "stdout", "text": "CPU times: user 32 ms, sys: 4 ms, total: 36 ms\nWall time: 25.8 s\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 64, "cell_type": "code", "source": "ss.read.parquet(\"/task1/example6\").show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+--------------------+-----------+\n|          categories|document_id|\n+--------------------+-----------+\n|Map(1510 -> 0.887...|    1059269|\n|Map(1408 -> 0.92,...|    1050604|\n|Map(1903 -> 0.92,...|    1472688|\n+--------------------+-----------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": 65, "cell_type": "code", "source": "# now we can join this table with events for instance\nss.read.parquet(\"/task1/example6\").registerTempTable(\"doc_categories_ready\")", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}, {"execution_count": 68, "cell_type": "code", "source": "ss.sql(\"\"\"\nselect \n    e.*, \n    dc.categories\nfrom \n    events as e\n    join doc_categories_ready as dc on dc.document_id = e.document_id\n\"\"\").show(3)", "outputs": [{"output_type": "stream", "name": "stdout", "text": "+----------+--------------+-----------+---------+--------+------------+--------------------+\n|display_id|          uuid|document_id|timestamp|platform|geo_location|          categories|\n+----------+--------------+-----------+---------+--------+------------+--------------------+\n|  18242074|e703634e3dfa39|    1000240|536236046|       2|          NG|Map(1503 -> 0.07,...|\n|  18694427|5b023d28c0a9f3|    1000240|687121504|       2|   US>MA>521|Map(1503 -> 0.07,...|\n|   3436070|55e1db49ff4eef|    1000240|223783698|       1|   US>CA>803|Map(1503 -> 0.07,...|\n+----------+--------------+-----------+---------+--------+------------+--------------------+\nonly showing top 3 rows\n\n"}], "metadata": {"collapsed": false, "editable": true, "deletable": true}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "editable": true, "deletable": true}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}}
